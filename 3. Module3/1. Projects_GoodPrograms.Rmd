---
title: "Using RStudio Projects and Writing Good Programs"
author: "Ashley I Naimi"
date: "Oct 2022"
urlcolor: blue
bibliography: ref.bib
link-citations: yes
output: 
    bookdown::pdf_book:
      base_format: tint::tintPdf
      toc: true
      number_sections: true
      includes:
        in_header: "../misc/preamble.tex"
      latex_engine: xelatex
    html_document:
      theme: readable
      toc: true
      toc_float: true
      number_sections: true
      css: "../misc/style.css"
---

```{r setup, include=FALSE}
library(knitr)
opts_chunk$set(tidy.opts=list(width.cutoff=40),tidy=TRUE)

packages <- c( "data.table","tidyverse","ggplot2","ggExtra","formatR","broom",
               "gridExtra","skimr","here","Hmisc","RColorBrewer")#,"gmm")

for (package in packages) {
  if (!require(package, character.only=T, quietly=T)) {
    install.packages(package, repos='http://lib.stat.cmu.edu/R/CRAN',dependencies=T)
  }
}

for (package in packages) {
  library(package, character.only=T)
}

remotes::install_github("rstudio/fontawesome")

library(fontawesome)

thm <- theme_classic() +
  theme(
    legend.position = "top",
    legend.background = element_rect(fill = "transparent", colour = NA),
    legend.key = element_rect(fill = "transparent", colour = NA)
  )
theme_set(thm)
```

\newpage
\onehalfspacing

\newpage
\onehalfspacing

# RStudio Projects

When carrying out a scientific project, there are many elements, skills, and practices that must come together to bring the project to a successful end. For example, as a project scientist, you or your team may be responsible for constructing a cohort and collecting data, processes and cleaning data, using statistical methods to analyze the data, and understanding how all of these steps combine to enable you to interpret the results. 

However, there is another set of skills that involve how to "build" and carry out a project from the ground up, while avoiding common pitfalls and sources of friction in carrying out and collaborating on a scientific analysis.

In this section, we'll discuss strategies and tools to optimize the scientific workflow and avoid these pitfalls. 

# Setting Up Your Project Workspace

The first step to carry out when starting a new project is to set up a folder structure that works for your team and meets the needs of your project. The approach one should take when setting up a project folder is summed up in the following slogan:

&nbsp;

"everything has a place and everything in its place"

&nbsp;

In following this slogan, we need to first establish a project workspace that has a "place for everything". This can include a place for data, code, figures, a manuscript or report to be written, a folder for some miscellaneous items that may be needed to structure or style the analytic results, and a place to try some things that may not be directly related to the analysis. 

To provide a practical example, consider that we have a folder on our computer named "Example_Project", which will be the location where we will carry out our analyses.^[This full example is available as a part of the course materials.] The needs we just listed suggest that a folder structure within the "Example_Project" folder such as the following could be of use:

```{r F7, out.width = "10cm", fig.align="center",echo=F}
knitr::include_graphics(here("figures", "FolderStructure.png"))
```

In the above Figure, the **code folder** contains all of the *source code files* needed deploy the analysis.^[recall the data science pipeline note from the previous section. In this "pipeline" framework, the key *real* elements of an analytic project are the source code files, and not the output that is generated from these files.] This folder can be considered the main or primary folder, because it contains a record of all the steps needed to produce (or reproduce) the results. 

The **data folder** contains everything that is needed for the code to run. For example, this folder could include the raw data, the analytic data, and then any other intermediary datasets needed to conduct the analysis of primary interest. 

The **figures folder** contains all images and figures relevant to the project (and generated from the source code). 

The **manuscript** or **report folder** contains the (e.g. Microsoft Word) document that will be used to report the findings to the wider scientific community, public, or other parties. 

The **misc folder** is a place that can contain all the other items needed to carry out the analysis that may not belong in any of the other folders. This can include notes from a course or website, email correspondence related to the project, .css files that allow you to style your results in a certain way, conference abstract submission instructions, and other relevant items.

The **sandbox folder** is a place to "play around" in. During the course of a project, you may come up with an idea that is either better than the one you are implementing, that may take the project in a different direction, or that may create a spin-off (i.e., second project) of the current project. The sandbox is a place where you can write code or generate data to explore this idea without affecting the structure of the main project folders.

Finally, the **README.md** file is a guide summarizing anything from the overall mission and goal of the project, down to the specific details of elements, items, code, figures, or whatever. The README file provides instructions for where things might deviate from a more traditional analytic protocol, or other special circumstances that users, collaborators, and/or reviewers of the code or project can become familiar with necessary details.

Once the project folder is created, we are ready to create an RStudio project that allows us to navigate this folder with the RStudio IDE.

:::{.rmdnote data-latex="{tip}"}

__The Importance of Folder Structure__:

|               Having a consistent and systematic approach to structuring project folders is beneficial for two reasons. First, if the team working on the project understands where things go and why, it makes collaboration more efficient as less time will be spent figuring out how things are organized. Similarly, there will inevitably be a time where you have to pause work on a project, and revisit it several weeks, months, or years later. Not having to spend time re-familiarizing yourself with the project landscape and folder structure can be equally efficient (i.e., collaborating with future self is easier). Second, having an organized project landscape makes it easier to avoid errors and facilitates reproducible scientific results. 

:::

# Projects and the `here` package

With the project folder structure created, the next step is to link it to an RStudio Project. To do this, we open up an RStudio session, and select

**File > New Project...**

In doing so, the following dialogue box should pop up:

```{r, out.width = "10cm", fig.align="center",echo=F}
knitr::include_graphics(here("figures", "NewProject1.png"))
```

After selecting "Existing Directory", this popup should change to the following:

```{r, out.width = "10cm", fig.align="center",echo=F}
knitr::include_graphics(here("figures", "NewProject2.png"))
```

After browsing to the location of the "Example_Project" folder, clicking "Create Project" will result in the following file being added to the project folder:

```{r, out.width = "10cm", fig.align="center",echo=F}
knitr::include_graphics(here("figures", "FolderStructure2.png"))
```

Double clicking on the `.Rproj` file will open an RStudio session that is isolated to the the `Example_Project` of interest. 

```{r, out.width = "18cm", fig.align="center",echo=F}
knitr::include_graphics(here("figures", "NewProject3.png"))
```

What's the benefit of using an RStudio Project, instead of just opening R? With RStudio projects, analysts can work on several projects at the same time, while keeping the directories, workspaces, histories, environments, and source documents separate between each project. In other words, each scientific project gets its own unique working environment. With proper coding practices,^[Including avoiding the use of `setwd()` and relying on the `here` package instead, as we will see below.] using RStudio can make collaboration a lot easier. 

# Writing Good R Programs in the Project Workspace

There are generally a few principles to follow when writing R programs in your project workspace. Let's first create a file we'll call `raw-data.R` that will allow us download the dataset of interest from the Web, and start cleaning it. Within this raw data file, we'll first ensure that all the packages we need are installed and loaded. 

As a general principle, it's always a good idea to include, at the top of your file, commands that you will need to execute the program. In our case, we include the following at the top of the `raw-data.R` program:

```{r}
packages <- c("tidyverse","here","VIM")

for (package in packages) {
  if (!require(package, character.only=T, quietly=T)) {
    install.packages(package, repos='http://lib.stat.cmu.edu/R/CRAN')
  }
}

for (package in packages) {
  library(package, character.only=T)
}
```

Next, we'll use the tidyverse `read_csv` function to access the NHEFS data from the following website:

[https://www.hsph.harvard.edu/miguel-hernan/causal-inference-book/](https://www.hsph.harvard.edu/miguel-hernan/causal-inference-book/)

```{r}

file_loc <- url("https://cdn1.sph.harvard.edu/wp-content/uploads/sites/1268/1268/20/nhefs.csv")
nhefs_data <- read_csv(file_loc)

# let's look at the data
dim(nhefs_data)

nhefs_data

```

Let's start with selecting the variables we would like to include in our analytic dataset, handle any missing data, and construct any derived variables. We can do this using the piping^[Note that the shortcut for adding a pipe in code on a Mac is `Command+Shift+M`, and on Windows is `Ctrl+Shirt+M`] structure and key functions in the tidyverse:

```{r}

nhefs_data <- nhefs_data %>% 
  select(seqn, qsmk, sbp, dbp, sex, age, race, income, wt82_71, death)

```

Let's look at missing data using the `aggr` function in the VIM package:

```{r}

aggr(nhefs_data)

```

We can also use a function we created to evaluate missingness:

```{r}

propMissing <- function(x){
  mean(is.na(x))
}

apply(nhefs_data,2,propMissing)

```

Next we can delete missing data to implement a complete case analysis with our analytic dataset:

```{r}
nhefs_data <- nhefs_data %>% na.omit()
```

And we can construct a new variable, mean arterial pressure (MAP), using our measures of systolic and diastolic blood pressure, and remove the blood pressure variables once MAP is created:

```{r}

nhefs_data <- nhefs_data %>% 
  mutate(map = dbp + (sbp - dbp)/3) %>% 
  select(-sbp, -dbp)

```

Finally, with our variables selected, missing data removed, and new variable created, we can explore our dataset^[In the Example Project code, I also show you how to generate a histogram for age, and save it to the figures folder using the `here` package, which is explained below.]:

```{r}

dim(nhefs_data)

nhefs_data %>% print(n=5)

```

Now that we've constructed our analytic dataset, it's time we save it to our data folder. The "traditional" way to do this is to start by setting the working directory. 

For example, in this pdf note package, which was generated in a project folder called "useR" on my computer, we could set the working directory as follows:

```{r}

getwd()

```
```{r, include=F}
og <- getwd()
```

From this, we can use the information to set the working directory to the "useR" folder. What this does is tell R that the root folder in which all project related directory changes should occur start in the "useR" folder. This way, we can simply save the analytic dataset we created with the `write_csv` function as in the following example:

```{r}

setwd("/Users/ain/Dropbox/Teaching/useR")

getwd()

write_csv(nhefs_data, file = "./data/analytic_data.csv")

```

```{r, include=F}
setwd(og)
```

The **major problem** with the above approach is that it makes the code completely dependent upon the computer on which it is being run. If I shared this code with a colleague, that person would have to scour my code files to change each instance of `setwd()` to the directory structure that's specific to their computer. This can create serious problems for collaboration and reproducibility.

Fortunately, there's another solution. In fact, the `here` package was developed precisely to address this issue. When we load the `here` package, we obtain the following information:

```{r}
here()
```

This package automatically sets the top level working directory to be the location of the RStudio project. With this package, we can then save our analytic dataset easily as:

```{r}

write_csv(nhefs_data, file = here("data","analytic_data.csv"))

```

# General Overview of the Process

Let's take a moment to consider the general approach we just took to construct an analytic dataset using the `raw-data.R` file. It may be useful to refer to the figure on the data science pipeline: 


```{r, out.width = "10cm", fig.align="center",fig.cap="The Data Science Pipeline",echo=F}
knitr::include_graphics(here("Figures","2022_09_20-reproducibility.pdf"))
```

Our `raw-data.R` file is basically an element of the "processing code" depicted in the Figure. What we did with this file is import some "raw" data, explore and manipulate, and save relevant items (analytic dataset, figures such as histograms) to their appropriate locations in teh folder structure.

This brings us to some general principles to follow when constructing a program file. First, our `raw-data.R` can be divided into three parts: 1) setup, where we install and load relevant libraries to do the work we need; 2) the code needed to carry out the work; and 3) the output section, where we save what we need to carry out the work.

\newpage

# References